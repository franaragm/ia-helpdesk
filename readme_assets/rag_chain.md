## ðŸ“Œ CÃ³digo completo del pipeline RAG actual

```python
rag_chain = (
    {
        "context": retriever | format_context,
        "question": RunnablePassthrough(),
    }
    | rag_prompt
    | llm_generation
    | StrOutputParser()
)
```

Esto **NO es magia**, es **composiciÃ³n de Runnables (LCEL) en LangChain moderno**.

---

## ðŸ§  Idea clave antes de empezar

En tu implementaciÃ³n:

* Todo es un **Runnable**
* `|` significa:
  ðŸ‘‰ *â€œla salida de la izquierda entra como input de la derechaâ€*

Piensa en esto como una **tuberÃ­a de datos**: la pregunta entra, los documentos se recuperan, se formatean, se pasa al prompt y finalmente el LLM devuelve texto.

---

## ðŸ§© 1ï¸âƒ£ El bloque inicial (diccionario)

```python
{
    "context": retriever | format_context,
    "question": RunnablePassthrough(),
}
```

### Â¿QuÃ© hace?

ðŸ‘‰ Es un **RunnableMap**: toma la entrada `query` y genera un diccionario:

```python
{
  "question": "consulta del usuario",
  "context": "texto de los documentos relevantes formateado"
}
```

---

### ðŸ”¹ `"question": RunnablePassthrough()`

* Recibe la pregunta del usuario.
* La pasa **tal cual** al prompt.
* Ejemplo:

```python
query = "Â¿CÃ³mo puedo resetear mi contraseÃ±a?"
output["question"] = "Â¿CÃ³mo puedo resetear mi contraseÃ±a?"
```

Sirve para que el prompt reciba la pregunta original.

---

### ðŸ”¹ `"context": retriever | format_context`

AquÃ­ estÃ¡ la **magia RAG** ðŸ”¥

#### Paso 1: `retriever`

```python
docs = retriever.invoke(query)  # List[Document]
```

Devuelve objetos `Document` con contenido y metadata:

```python
[
  Document(page_content="Para resetear tu contraseÃ±a ...", metadata={"filename":"manual.pdf"}),
  Document(page_content="Sigue estos pasos ...", metadata={"filename":"faq.pdf"}),
]
```

#### Paso 2: `| format_context`

Convierte los `Document` en texto plano legible para el prompt, aÃ±ade encabezados y fuentes:

```
[Document 1] - Source: manual.pdf
Para resetear tu contraseÃ±a ...

[Document 2] - Source: faq.pdf
Sigue estos pasos ...
```

Se asigna a `"context"` en el diccionario.

---

### âœ… Resultado del bloque inicial

Si la pregunta es:

```
"Â¿CÃ³mo puedo resetear mi contraseÃ±a?"
```

El output serÃ¡:

```python
{
  "question": "Â¿CÃ³mo puedo resetear mi contraseÃ±a?",
  "context": "[Document 1]...\n\n[Document 2]..."
}
```

---

## ðŸ§© 2ï¸âƒ£ `| rag_prompt`

```python
| rag_prompt
```

El prompt RAG espera:

```
FRAGMENTOS DE SOPORTE:
{context}

PREGUNTA: {question}
```

LangChain reemplaza automÃ¡ticamente `{context}` y `{question}` y genera un **string listo para el LLM**.

---

## ðŸ§© 3ï¸âƒ£ `| llm_generation`

```python
| llm_generation
```

* EnvÃ­a el prompt al modelo (OpenAI, etc.)
* Devuelve la respuesta generada

Ejemplo conceptual:

```python
AIMessage(content="Para resetear tu contraseÃ±a, sigue estos pasos...")
```

---

## ðŸ§© 4ï¸âƒ£ `| StrOutputParser()`

* Extrae solo **texto plano**
* Elimina metadata o envoltorios del LLM

Resultado final:

```python
"Para resetear tu contraseÃ±a, sigue estos pasos..."
```

---

## ðŸ§  Diagrama mental completo

```
Pregunta del usuario
   â”‚
   â–¼
RunnableMap {
  question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "Â¿CÃ³mo puedo resetear mi contraseÃ±a?"
  context  â”€â–º retriever â”€â–º docs â”€â–º format_context â”€â–º texto
}
   â”‚
   â–¼
rag_prompt (inyecta context + question)
   â”‚
   â–¼
LLM (genera respuesta)
   â”‚
   â–¼
StrOutputParser
   â”‚
   â–¼
Respuesta final (str)
```

---

## ðŸ§ª Â¿QuÃ© devuelve `query_rag(query)`?

```python
answer_obj: RagAnswer = query_rag("Â¿CÃ³mo puedo resetear mi contraseÃ±a?")
```

`RagAnswer` incluye:

* `answer` â†’ texto final para mostrar al usuario
* `confidence` â†’ heurÃ­stica de confiabilidad
* `sources` â†’ lista de archivos que respaldan la respuesta

> Nota: **los documentos originales (`Document`) se recuperan por separado con `retriever.invoke(query)`** para mostrar fragmentos en la UI.

---

## ðŸ§  Por quÃ© esta arquitectura es buena

âœ… SeparaciÃ³n clara:

* retrieval (recuperar documentos)
* formatting (contexto legible)
* prompting (prompt RAG)
* generation (LLM)

âœ… FÃ¡cil de extender:

* reranking
* filtros
* explicaciones adicionales

âœ… Transparente:

* `query_rag` da respuesta final
* `retriever.invoke` da trazabilidad de documentos

---

## ðŸ’¡ AnalogÃ­a

* `retriever` â†’ biblioteca: devuelve los libros relevantes
* `format_context` â†’ resumen legible de los libros
* `rag_chain` â†’ abogado: lee los libros, genera respuesta
* `query_rag` â†’ funciÃ³n que entrega **respuesta + confianza + fuentes** al usuario

---
