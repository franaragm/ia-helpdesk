## ğŸ“Œ El cÃ³digo completo

```python
rag_chain = (
    {
        "context": retriever | _format_docs,
        "question": RunnablePassthrough(),
    }
    | rag_prompt
    | llm_generation
    | StrOutputParser()
)
```

Esto **NO es magia**, es **composiciÃ³n de Runnables** en LangChain.

---

## ğŸ§  Idea clave antes de empezar

En LangChain moderno:

* Todo es un **Runnable**
* `|` significa:
  ğŸ‘‰ *â€œla salida de la izquierda entra como input de la derechaâ€*

Piensa en esto como una **tuberÃ­a de datos**.

---

## ğŸ§© 1ï¸âƒ£ El bloque inicial (diccionario)

```python
{
    "context": retriever | _format_docs,
    "question": RunnablePassthrough(),
}
```

### Â¿QuÃ© es esto?

ğŸ‘‰ Es un **RunnableMap**
Convierte **un solo input** (la pregunta) en **un diccionario estructurado**.

---

### ğŸ”¹ `"question": RunnablePassthrough()`

```python
"question": RunnablePassthrough()
```

* Recibe el input original (la pregunta)
* Lo devuelve **tal cual**
* Sirve para pasar la pregunta al prompt

Ejemplo:

```python
input = "Â¿QuiÃ©n es el arrendatario?"
output["question"] = "Â¿QuiÃ©n es el arrendatario?"
```

---

### ğŸ”¹ `"context": retriever | _format_docs`

AquÃ­ estÃ¡ la **magia RAG** ğŸ”¥

#### Paso 1: `retriever`

```python
retriever.invoke(question) -> List[Document]
```

Devuelve algo asÃ­:

```python
[
  Document(page_content="El arrendatario es Juan PÃ©rez...", metadata={...}),
  Document(page_content="Contrato firmado el 3 de mayo...", metadata={...})
]
```

---

#### Paso 2: `| _format_docs`

```python
retriever | _format_docs
```

* Toma la lista de `Document`
* Los convierte en **texto legible**
* AÃ±ade fuentes, pÃ¡ginas, numeraciÃ³n

Resultado final:

```text
[Fragmento 1] - Fuente: contrato1.pdf - PÃ¡gina: 2
El arrendatario es Juan PÃ©rez...

[Fragmento 2] - Fuente: contrato2.pdf - PÃ¡gina: 1
Contrato firmado el 3 de mayo...
```

ğŸ‘‰ Eso se asigna a la clave `"context"`.

---

### âœ… Resultado del bloque completo

Si la pregunta es:

```
"Â¿QuiÃ©n es el arrendatario?"
```

El output de este bloque serÃ¡:

```python
{
  "question": "Â¿QuiÃ©n es el arrendatario?",
  "context": "[Fragmento 1]...\n\n[Fragmento 2]..."
}
```

---

## ğŸ§© 2ï¸âƒ£ `| rag_prompt`

```python
| rag_prompt
```

Tu prompt es:

```text
FRAGMENTOS DE CONTRATOS:
{context}

PREGUNTA: {question}
```

LangChain hace automÃ¡ticamente:

```python
rag_prompt.format(
    context=context,
    question=question
)
```

ğŸ‘‰ Resultado: **un string listo para el LLM**.

---

## ğŸ§© 3ï¸âƒ£ `| llm_generation`

```python
| llm_generation
```

* EnvÃ­a el prompt al modelo (OpenAI / Groq / etc.)
* Devuelve la respuesta del LLM (objeto o mensaje)

Ejemplo conceptual:

```python
AIMessage(
  content="El arrendatario es Juan PÃ©rez..."
)
```

---

## ğŸ§© 4ï¸âƒ£ `| StrOutputParser()`

```python
| StrOutputParser()
```

* Extrae solo el **texto plano**
* Elimina metadata del mensaje

Resultado final:

```python
"El arrendatario es Juan PÃ©rez..."
```

---

## ğŸ§  Diagrama mental completo

```
Pregunta
   â”‚
   â–¼
RunnableMap {
  question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "Â¿QuiÃ©n es el arrendatario?"
  context  â”€â–º retriever â”€â–º docs â”€â–º _format_docs â”€â–º texto
}
   â”‚
   â–¼
rag_prompt (inyecta context + question)
   â”‚
   â–¼
LLM (genera respuesta)
   â”‚
   â–¼
StrOutputParser
   â”‚
   â–¼
Respuesta final (str)
```

---

## ğŸ§ª Â¿QuÃ© devuelve `rag_chain.invoke()`?

```python
answer: str = rag_chain.invoke("Â¿QuiÃ©n es el arrendatario?")
```

ğŸ‘‰ **Solo la respuesta**, no los documentos
(por eso los recuperas aparte para la UI).

---

## ğŸ§  Por quÃ© esta arquitectura es MUY buena

âœ… SeparaciÃ³n clara:

* retrieval
* formatting
* prompting
* generation

âœ… FÃ¡cil de extender:

* aÃ±adir reranking
* aÃ±adir filtros
* aÃ±adir explicaciones

âœ… 100% compatible con LangChain moderno

---



## **una diferencia conceptual entre â€œrecuperar documentosâ€ y â€œgenerar respuestaâ€ en RAG**.

Vamos paso a paso:

---

### 1ï¸âƒ£ Lo que hace `rag_chain.invoke(question)`

En tu pipeline:

```python
rag_chain = (
    {
        "context": retriever | _format_docs,
        "question": RunnablePassthrough(),
    }
    | rag_prompt
    | llm_generation
    | StrOutputParser()
)
```

Cuando ejecutas:

```python
answer = rag_chain.invoke(question)
```

* **Internamente**, el pipeline hace:

  1. `retriever.invoke(question)` â†’ obtiene los documentos
  2. `_format_docs` â†’ los convierte en texto
  3. Inserta ese texto en `rag_prompt`
  4. Llama al LLM (`llm_generation`)
  5. Extrae el texto final (`StrOutputParser`)

âœ… El resultado `answer` **ya incluye la informaciÃ³n de los documentos**, pero **no tienes acceso a los objetos Document originales**.

---

### 2ï¸âƒ£ Por quÃ© necesitas invocar el retriever por separado

```python
docs = retriever.invoke(question)
```

* Esto te da **los objetos `Document` reales**.
* Incluyen metadata como:

  * `source` (archivo)
  * `page` (pÃ¡gina)
  * `chunk_id`
* Que luego usas para mostrar los fragmentos en la UI o para **log/traceability**.

Si solo usaras `rag_chain.invoke()`, **solo tendrÃ­as texto plano**, sin saber de dÃ³nde vino cada fragmento.

---

### 3ï¸âƒ£ Ejemplo conceptual

Pregunta:

```
"Â¿QuiÃ©n es el arrendatario?"
```

3a) `rag_chain.invoke(question)` â†’ `answer`

```
"El arrendatario es Juan PÃ©rez..."
```

* Ãštil para mostrar al usuario
* No te dice **quÃ© documento / pÃ¡gina** respalda la respuesta

3b) `retriever.invoke(question)` â†’ `docs`

```
[
  Document(page_content="El arrendatario es Juan PÃ©rez", metadata={"source":"contrato1.pdf", "page":2}),
  Document(page_content="Contrato firmado...", metadata={"source":"contrato2.pdf", "page":1})
]
```

* Ãštil para mostrar **fragmentos**, referencias y auditorÃ­a
* Te permite construir UI â€œfragmento por fragmentoâ€ (lo que haces en tu columna derecha)

---

### 4ï¸âƒ£ Por quÃ© no se combinan directamente

PodrÃ­as intentar:

```python
answer, docs = rag_chain.invoke_and_return_docs(question)
```

Pero **LangChain no tiene un mÃ©todo estÃ¡ndar asÃ­**.
Separar **retrieval** y **generation** te da:

* Flexibilidad
* Mejor trazabilidad
* Posibilidad de **re-ranking** o post-procesamiento antes de la generaciÃ³n

---

### 5ï¸âƒ£ Resumen conceptual

| AcciÃ³n                                      | MÃ©todo                       | Resultado                   | Uso en tu app                |
| ------------------------------------------- | ---------------------------- | --------------------------- | ---------------------------- |
| Recuperar documentos relevantes             | `retriever.invoke(question)` | List[Document] con metadata | Mostrar fragmentos en UI     |
| Generar respuesta basada en esos documentos | `rag_chain.invoke(question)` | str (texto de LLM)          | Mostrar respuesta al usuario |

---

ğŸ’¡ **AnalogÃ­a:**

* `retriever` â†’ biblioteca â†’ te da los libros
* `rag_chain` â†’ abogado â†’ lee los libros y te responde
* Necesitas **los libros y la respuesta** para que todo sea transparente y auditable.

---
